from google.colab import drive
drive.mount('/content/drive', force_remount=True)
     
Mounted at /content/drive

import os
import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, recall_score, accuracy_score, f1_score, precision_score
from sklearn.utils.class_weight import compute_class_weight

from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import matplotlib.pyplot as plt
%matplotlib inline

from prettytable import PrettyTable
     

# Specify GPU
device = torch.device("cuda")
     

# Change to your own directory
try: 
    os.chdir("/content/drive/MyDrive/BT4222 Project")
    print("Directory changed")
except OSError:
    print("Error: Can't change the Current Working Directory")
     
Directory changed
Define constants

# Define constants
EPOCHS = 5
BATCH_SIZE = 32
LEARNING_RATE = 1e-5
SEED = 4222
     
Load dataset

# Load dataset
suicide_detection_df = pd.read_csv('Data/suicide_detection_final_cleaned.csv', header=0)
suicide_detection_df.drop(columns=['text'], axis=1, inplace=True)
suicide_detection_df = suicide_detection_df.rename(columns={"cleaned_text": "text"})
classes = {"suicide": 1, "non-suicide": 0}
suicide_detection_df = suicide_detection_df.replace({"class": classes})
suicide_detection_df.head()
     
class	text
0	1	sex wife threaten suicide recently leave wife ...
1	0	weird not affect compliment come know real lif...
2	0	finally hear bad year swear fucking god annoying
3	1	need help just help cry hard
4	1	end tonight not anymore quit
Train-Test-Val Split

# Split dataset into train, validation and test sets
train_text, temp_text, train_labels, temp_labels = train_test_split(suicide_detection_df['text'], suicide_detection_df['class'],
                                                                    random_state=SEED,
                                                                    test_size=0.2,
                                                                    stratify=suicide_detection_df['class'])

val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,
                                                                random_state=SEED,
                                                                test_size=0.5,
                                                                stratify=temp_labels)
     
Max Length and Number of vocab words in Train set

max_length = max([len(s.split()) for s in train_text])
max_length
     
62

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_text)

vocab_size = len(tokenizer.word_index) + 1
     
Padding

def tokenize_and_encode(text, max_length=62):
    """Tokenize and encode sequences."""

    # sequence encode
    encoded_docs = tokenizer.texts_to_sequences(text)
    # pad sequences
    padded_sequence = pad_sequences(encoded_docs, maxlen=max_length, padding='post')

    return padded_sequence

# Tokenize and encode sequences in all datasets
tokens_train = tokenize_and_encode(train_text)
tokens_val = tokenize_and_encode(val_text)
tokens_test = tokenize_and_encode(test_text)
     

# create Tensor datasets
train_data = TensorDataset(torch.from_numpy(tokens_train), torch.from_numpy(train_labels.to_numpy()))
val_data = TensorDataset(torch.from_numpy(tokens_val), torch.from_numpy(val_labels.to_numpy()))

# Sampler for sampling the data
train_sampler = RandomSampler(train_data)
val_sampler = SequentialSampler(val_data)

# DataLoader
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)
     
Load Word Embedding

# load embedding as a dict
def load_embedding(filename):
	# load embedding into memory, skip first line
	file = open(filename,'r')
	lines = file.readlines()[1:]
	file.close()
	# create a map of words to vectors
	embedding = dict()
	for line in lines:
		parts = line.split()
		# key is string word, value is numpy array for vector
		embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')
	return embedding
     

# create a weight matrix for the Embedding layer from a loaded embedding
def get_weight_matrix(embedding, vocab, embedding_dim):
	# total vocabulary size plus 0 for unknown words
	vocab_size = len(vocab) + 1
	# define weight matrix dimensions with all 0
	weight_matrix = np.zeros((vocab_size, embedding_dim))
	# step vocab, store vectors using the Tokenizer's integer mapping
	for word, i in vocab.items():
		weight_matrix[i] = embedding.get(word)

	return weight_matrix
     

def create_emb_layer(weights_matrix, non_trainable=False):
    num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]
    emb_layer = nn.Embedding(num_embeddings, embedding_dim)
    emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})
    # emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix))
    if non_trainable:
        emb_layer.weight.requires_grad = False

    return emb_layer, num_embeddings, embedding_dim
     

# load word2vec embedding from file
raw_embedding_word2vec = load_embedding('Data/embedding_word2vec.txt')
# get vectors in the right order
embedding_vectors_word2vec = get_weight_matrix(raw_embedding_word2vec, tokenizer.word_index, 300)
embedding_vectors_word2vec = np.float32(embedding_vectors_word2vec)
     

# load glove embedding from file
raw_embedding_glove = load_embedding('Data/glove_twitter_27B_200d.txt')
     

# get vectors in the right order
embedding_vectors_glove = get_weight_matrix(raw_embedding_glove, tokenizer.word_index, 200)
embedding_vectors_glove = np.float32(embedding_vectors_glove)
     

for arr in embedding_vectors_glove:
    for idx, i in enumerate(arr):
        if np.isnan(arr[idx]):
            arr[idx] = 0
     
CNN Model Architecture

class CNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim, 
                 n_filters, filter_sizes, output_dim,
                 dropout_rate, pre_trained=False, embedding_vectors=None):
        
        super().__init__()
        
        if pre_trained:
            self.embedding, num_embeddings, embedding_dim = create_emb_layer(embedding_vectors, True)
        else:
            # Create word embeddings from the input words
            self.embedding = nn.Embedding(vocab_size, embedding_dim)
        
        # Note: ModuleList holds a list of PyTorch nn.Modules.
        # Allowing us to pass in n number of filter sizes, thereby creating a convolutional layer for each of them
        self.convs = nn.ModuleList([
                                    nn.Conv1d(in_channels = embedding_dim, 
                                              out_channels = n_filters, 
                                              kernel_size = fs)
                                    for fs in filter_sizes
                                    ])
        
        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)
        
        self.dropout = nn.Dropout(dropout_rate)
        
    def forward(self, text): 
        '''
        Iterates through the filter size applied to each convolutional layer to get convolutional output,
        then apply max pooling before concatenating together and passing through the dropout and fully connected layers.

        text: contains [batch size, length of sentence]
        '''
        embedded = self.embedding(text) # [batch size, length of sentence, embedding dimension]
        
        embedded = embedded.permute(0, 2, 1) # rearrange to get [batch size, embedding dimension, length of  sentence]
    
        conved = [F.relu(conv(embedded)) for conv in self.convs] # [batch size, n_filters, length of sentence - filter_size of convu layer + 1]
        
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # [batch size, n_filters] for each convu layer
        
        cat = self.dropout(torch.cat(pooled, dim = 1)) # [batch size, n_filters * len(filter_sizes)]
            
        return self.fc(cat)
     

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
     

# Instantiate the model w/ hyperparams
embedding_dim = 300
n_filters = 32
filter_sizes = [5,6,7,8]
output_dim = 1
dropout_rate = 0.2
     

# Define the loss function
criterion = nn.BCEWithLogitsLoss()

# push to GPU
criterion = criterion.to(device)
     
Model 1: No pre trained embedding weights

model1 = CNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_rate, pre_trained=False)

print("No pre trained embedding weights")
print(model1)
print(f'Model 1 has {count_parameters(model1):,} trainable parameters')
     
No pre trained embedding weights
CNN(
  (embedding): Embedding(31130, 300)
  (convs): ModuleList(
    (0): Conv1d(300, 32, kernel_size=(5,), stride=(1,))
    (1): Conv1d(300, 32, kernel_size=(6,), stride=(1,))
    (2): Conv1d(300, 32, kernel_size=(7,), stride=(1,))
    (3): Conv1d(300, 32, kernel_size=(8,), stride=(1,))
  )
  (fc): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
Model 1 has 9,588,857 trainable parameters
Model 2: Word2Vec pre trained embedding weights

model2 = CNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout_rate, 
             pre_trained=True, embedding_vectors=embedding_vectors_word2vec)
model2.embedding.weight.data.copy_(torch.from_numpy(embedding_vectors_word2vec))

print("With Word2Vec pre trained embedding weights")
print(model2)
print(f'Model 2 has {count_parameters(model2):,} trainable parameters')
     
With Word2Vec pre trained embedding weights
CNN(
  (embedding): Embedding(31130, 300)
  (convs): ModuleList(
    (0): Conv1d(300, 32, kernel_size=(5,), stride=(1,))
    (1): Conv1d(300, 32, kernel_size=(6,), stride=(1,))
    (2): Conv1d(300, 32, kernel_size=(7,), stride=(1,))
    (3): Conv1d(300, 32, kernel_size=(8,), stride=(1,))
  )
  (fc): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
Model 2 has 249,857 trainable parameters
Model 3: gloVe Twitter dataset (200d) pre trained embedding weights

model3 = CNN(vocab_size, 200, n_filters, filter_sizes, output_dim, dropout_rate, 
             pre_trained=True, embedding_vectors=embedding_vectors_glove)
model3.embedding.weight.data.copy_(torch.from_numpy(embedding_vectors_glove))

print("With gloVe pre trained embedding weights")
print(model3)
print(f'Model 3 has {count_parameters(model3):,} trainable parameters')
     
With gloVe pre trained embedding weights
CNN(
  (embedding): Embedding(31130, 200)
  (convs): ModuleList(
    (0): Conv1d(200, 32, kernel_size=(5,), stride=(1,))
    (1): Conv1d(200, 32, kernel_size=(6,), stride=(1,))
    (2): Conv1d(200, 32, kernel_size=(7,), stride=(1,))
    (3): Conv1d(200, 32, kernel_size=(8,), stride=(1,))
  )
  (fc): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
Model 3 has 166,657
